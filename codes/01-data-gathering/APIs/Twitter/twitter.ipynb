{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for drafting the twitter.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting tweets for incident ID 20220620ILGRC...\n",
      "Requesting tweets for incident ID 20220613WAMAE...\n",
      "All requests complete.\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Read API key credentials\n",
    "api = pd.read_json('~/Coding/APIs/twitter_keys.json')['DSAN_501-project']\n",
    "\n",
    "# Save credentials\n",
    "consumer_key=api.loc[\"key\"]\n",
    "consumer_secret=api.loc[\"key_secret\"]\n",
    "access_token=api.loc[\"access_token\"]\n",
    "access_token_secret=api.loc[\"access_token_secret\"]\n",
    "bearer_token=api.loc[\"bearer_token\"]\n",
    "\n",
    "# PREPARE AND ORGANIZE DATA\n",
    "# Read in dataset\n",
    "incidents = pd.read_csv('../../../../data/02-clean-data/SSDB/incident.csv')\n",
    "incidents['Date'] = pd.to_datetime(incidents['Date'])\n",
    "\n",
    "# Filter out incidents that occurred before 2006-03-21 (day of first ever tweet)\n",
    "incidents_twitter = incidents.copy()[incidents['Date'] > pd.to_datetime('2006-03-21 15:50:00')]\n",
    "\n",
    "# Add new data column marking one week after the incident\n",
    "incidents_twitter['Date_Week_Later'] = incidents_twitter['Date'] + pd.Timedelta(days=7)\n",
    "\n",
    "# Convert date columns to YYYY-MM-DDTHH:mm:ssZ (ISO 8601/RFC 3339) for API requests\n",
    "incidents_twitter['Query_Date'] = incidents_twitter['Date'].dt.strftime('%Y-%m-%d') + \"T00:00:00Z\"\n",
    "incidents_twitter['Query_Date_Week_Later'] = incidents_twitter['Date_Week_Later'].dt.strftime('%Y-%m-%d') + \"T00:00:00Z\"\n",
    "\n",
    "# Save incidents_twitter as csv\n",
    "incidents_twitter.to_csv('./../../../../data/01-modified-data/SSDB/incident_twitter.csv')\n",
    "\n",
    "# Replace & with and in school names\n",
    "new_schools = [str(school).replace('&', 'and') for school in incidents_twitter['School']]\n",
    "\n",
    "# Construct get request parameters, save IDs to list\n",
    "ids = incidents_twitter['Incident_ID']\n",
    "queries = \"-is:retweet ((school (shooting OR shootings)) OR (\" + incidents_twitter['City'] + \" (shooting OR shootings)) OR (\" + incidents_twitter['School'] + \"))\"\n",
    "start_times = incidents_twitter['Query_Date']\n",
    "end_times = incidents_twitter['Query_Date_Week_Later']\n",
    "\n",
    "# Specify relative filepath for outputs\n",
    "rel_filepath = \"../../../../data/00-raw-data/Twitter/\"\n",
    "\n",
    "\n",
    "# DEFINE FUNCTIONS FOR API REQUESTS\n",
    "def generate_url(query, start_time, end_time, max_results=500, next_token=False):\n",
    "    if (next_token == False):\n",
    "        url = \"https://api.twitter.com/2/tweets/search/all?query={}&start_time={}&end_time={}&tweet.fields=text,author_id,created_at,geo,lang&max_results={}\".format(query, start_time, end_time, max_results)\n",
    "    else:\n",
    "        url = \"https://api.twitter.com/2/tweets/search/all?next_token={}&query={}&start_time={}&end_time={}&tweet.fields=text,author_id,created_at,geo,lang&max_results={}\".format(next_token, query, start_time, end_time, max_results)\n",
    "    return url\n",
    "\n",
    "headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "\n",
    "def search_twitter(url):\n",
    "    response = requests.request(\"GET\", url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "num_requests = 0\n",
    "num_results = 0\n",
    "\n",
    "max_inc_results = 600 # 7000\n",
    "\n",
    "# 2 limits:\n",
    "# 1. 300 requests / 15 min\n",
    "# 2. 1 request / sec\n",
    "\n",
    "# BEGIN API REQUESTS\n",
    "for i, (inc_id, query, start_t, end_t) in enumerate(zip(ids[0:2], queries[0:2], start_times[0:2], end_times[0:2])):\n",
    "    # Print status\n",
    "    print(\"Requesting tweets for incident ID \" + inc_id + \"...\")\n",
    "\n",
    "    # Initialize tracking variables\n",
    "    next_page = True # Track pagination\n",
    "    num_page = 0 # Track pagination numbers\n",
    "    nt = False # Set nt (next token) as False for first request of query\n",
    "\n",
    "    # Start \"timers\" for regulation of limits\n",
    "    limit1_start = time.time() # Do not exceed 300 requests / 15 min\n",
    "    limit2_start = time.time() # Do not exceed 1 request / second\n",
    "\n",
    "    while (next_page == True):\n",
    "        # Generate URL (nt will be false on first and last requests of the query)\n",
    "        query_url = generate_url(query=query, start_time=start_t, end_time=end_t, max_results=500, next_token=nt)\n",
    "\n",
    "\n",
    "        # Make request\n",
    "        json_response = search_twitter(url=query_url)\n",
    "\n",
    "        # Track metadata\n",
    "        num_results = num_results + json_response['meta']['result_count']\n",
    "        try:\n",
    "            nt = json_response['meta']['next_token']\n",
    "            num_page += 1\n",
    "        except KeyError:\n",
    "            nt = False\n",
    "            next_page = False\n",
    "\n",
    "        # Track progress\n",
    "        num_requests += 1\n",
    "\n",
    "        # Save response\n",
    "        output = json.dumps(json_response, indent=4, sort_keys=True)\n",
    "\n",
    "        with open(rel_filepath + inc_id + '_' + str(num_page) + '.json', 'w') as outfile:\n",
    "            outfile.write(output)\n",
    "\n",
    "        # Check limits\n",
    "        # Limit 1: 300 requests / 15 min\n",
    "        if (num_requests >= 300) or ((time.time() - limit1_start) > 14*60):\n",
    "            # Sleep ten minutes (1 request / sec for 300 requests is 5 min)\n",
    "            print(\"10 minute sleep\")\n",
    "            for i in tqdm(range(10*60)):\n",
    "                time.sleep(1)\n",
    "            \n",
    "            # Reset request count and \"timer\"\n",
    "            num_requests = 0\n",
    "            limit1_start = time.time()\n",
    "\n",
    "        # Limit 2: 1 request / sec\n",
    "        if (time.time() - limit2_start <= 1):\n",
    "            time.sleep(1 - (time.time() - limit2_start) )\n",
    "\n",
    "        limit2_start = time.time() # Reset \"timer\"\n",
    "\n",
    "        # Check if tweet count has exceeded max_inc_results\n",
    "        if (max_inc_results < num_results):\n",
    "            next_page = False\n",
    "            num_results = 0\n",
    "        \n",
    "\n",
    "\n",
    "print(\"All requests complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1184"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids.tolist().index('20210921OHMIM') # Check this incident at end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "917"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids.tolist().index('20140911UTWET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404     20210104NYPEJ\n",
       "405     20201229WAROS\n",
       "406     20201225FLYOP\n",
       "407     20201216ILDUC\n",
       "408     20201211VATAY\n",
       "            ...      \n",
       "1179    20060522SCBUI\n",
       "1180    20060505FLPAM\n",
       "1181    20060424NCEAC\n",
       "1182    20060418TXWEH\n",
       "1183    20060405DCROW\n",
       "Name: Incident_ID, Length: 780, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids[404:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-is:retweet ((school (shooting OR shootings)) OR (Chicago (shooting OR shootings)) OR (Lindblom Math & Science Academy High School))'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries[509]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lindblom Math and Science Academy High School'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_schools = [str(school).replace('&', 'and') for school in incidents_twitter['School']]\n",
    "\n",
    "new_schools[509]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('anly501')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2df872c4a1806b14b9cbf083c08c99d2077b5ff2992734c586bc376c177358b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
