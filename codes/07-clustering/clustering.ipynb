{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our previous analysis of tweets relating to school shootings have been limited to tweets containing hashtags. We operated under the assumption that a hashtag could act as a proxy for a label by grouping hashtags commonly associated with news tweets and grouping hashtags commonly associated with tweets expressing opinions. Clearly this approach has several shortcomings--most notably the lack of practical significance.  \n",
    "  \n",
    "In this analysis, we will perform clustering to identify groups in the tweets that hopefully transcend the less-interesting binary classification of tweets as being news-oriented or opinion-oriented. We expect that our clustering analysis will split tweets by their emotional sentiment (perhaps empathetic/critical) or by their political orientation (tighter gun laws / looser gun laws)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is a field of unsupervised machine learning that groups observations in data based on how closely they are located to each other. As a brief introduction, we discuss a few different clustering techniques below:  \n",
    "\n",
    "### K-Means\n",
    "Not to be confused with K Nearest Neighbors, K-Means is a clustering algorithm that accepts a parameter $K$ determining the number of clusters to identify in the data. For each cluster, a centroid is determined which serves as the arbitrary center of the cluster. The centroid can be calculated using mean, median, or other metrics to determine the center of the observations.  \n",
    "\n",
    "The algorithm begins by selecting K random arbitrary centroids in the space. From there, it iteratively updates to represent the optimal center for K clusters in the dataset based on some distance metric (usually Euclidean distance). Once the iterations no longer update the centroids, the centroids are considered to have converged.\n",
    "\n",
    "K-Means clustering is ideal for clustering data that follow a Gaussian distribution. Gaussian distributions appear as a circle or oval in 2-dimensional space and as spherical or ovaline clouds in 3-dimensional space. Dimensions greater than three can be difficult to visualize, so often the best approach for determining the optimal clustering algorithm is to try multiple ones.\n",
    "\n",
    "### Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\n",
    "DBSCAN is an alternative clustering algorithm that is based on grouping observations by their density. In this case, density refers to the concentration of observations in a relatively small area of the space. In DBSCAN, outliers are omitted from clusters because they are, by definition, isolated from high-density clusters of data.\n",
    "\n",
    "The fundamental difference between DBSCAN and K-Means is that DBSCAN is very good at identifying clusters of data that are not linear / do not follow a Gaussian distribution. Consider the graphic below, published by Scikit Learn, illustrating the results of various clustering algorithms on toy datasets of different shapes.\n",
    "\n",
    "![\"https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html\"](./../../501-project-website/images/clustering/scikit-learn_clustering.png)\n",
    "\n",
    "As we can see, DBSCAN is much better at clustering irregular shapes than K-Means.\n",
    "\n",
    "### Hierarchical Clustering\n",
    "Hierarchical clustering, unlike the clustering algorithms defined above (which are considered partitional clustering algorithms), is an approach to clustering that assigns data to different \"sub-clusters\" at different levels of the hierarchy. For example, at the highest levels, the data are separated into few clusters, but the clusters are large. At the lower levels of the hierarchy, the clusters get smaller but are more plentiful.\n",
    "\n",
    "Oftentimes, the size and number of clusters at each hierarchy can effectively be illustrated in a dendrogram. This approach allows the user to determine the optimal number/size of clusters \"at a glance\"--depending on the use case of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Selection\n",
    "\n",
    "Because we do not consider any labels in clustering, we are free to use the entire set of tweets in this analysis. Using [this](\"https://github.com/anly501/anly-501-project-joshegladwell/blob/main/codes/07-clustering/bow_generator.py\") Python script, we attempt to create a bag of words model for a random selection of 50,000 tweets (rather than only tweets with hashtags). However, due to computing limitations we will settle for the same BOW model that we have used in our previous analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "# Read in data\n",
    "df_train = pd.read_csv('./../../../large_data/03-classification-model-data/Twitter/df_train.tsv', sep='\\t')\n",
    "df_test = pd.read_csv('./../../../large_data/03-classification-model-data/Twitter/df_test.tsv', sep='\\t')\n",
    "\n",
    "bow_train = np.array(pd.read_csv('./../../../large_data/03-classification-model-data/Twitter/bow_train.csv'))\n",
    "bow_test = np.array(pd.read_csv('./../../../large_data/03-classification-model-data/Twitter/bow_test.csv'))\n",
    "\n",
    "vocab_train = json.load(open('./../../../large_data/03-classification-model-data/Twitter/vocab_train.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31524, 4)\n",
      "(7881, 4)\n",
      "(39405, 4)\n",
      "(31524, 28955)\n",
      "(7881, 28955)\n",
      "(39405, 28955)\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the train and test sets\n",
    "df = pd.concat([df_train, df_test])\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "print(df.shape)\n",
    "\n",
    "bow = np.concatenate((bow_train, bow_test))\n",
    "print(bow_train.shape)\n",
    "print(bow_test.shape)\n",
    "print(bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevent libraries for clustering. we will use KMeans, AgglomerativeClustering, MeanShift, Birch, and DBSCAN\n",
    "from statistics import mode\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<__array_function__ internals>:2\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'sklearn.cluster._k_means_common._relocate_empty_clusters_dense'\n",
      "Traceback (most recent call last):\n",
      "  File \"<__array_function__ internals>\", line 2, in where\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# for k means clustering we will use the elbow method to find the optimal number of clusters. we will use the inertia_ attribute to find the sum of squared distances of samples to their closest cluster center. we will use the range of 1 to 10 clusters. plot the inertia_ values for each number of clusters. make sure to save it in a dataframe and plot it using matplotlib.\n",
    "distortions = []\n",
    "inertias = []\n",
    "k = 11\n",
    "\n",
    "for k in range(1, k):\n",
    "    print(\"K=\",k)\n",
    "    kmeanModel = KMeans(n_clusters=k, init='k-means++')\n",
    "    kmeanModel.fit(bow)\n",
    "\n",
    "    distortions.append(sum(np.min(cdist(bow, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / bow.shape[0])\n",
    "    inertias.append(kmeanModel.inertia_)\n",
    "\n",
    "evaluation = pd.DataFrame.from_records({'Cluster': np.arange(1, k+1), 'Distortion': distortions, 'Inertia': inertias})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('anly501')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2df872c4a1806b14b9cbf083c08c99d2077b5ff2992734c586bc376c177358b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
